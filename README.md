# SurveilNet â€“ Trade Surveillance & Anomaly Detection Platform

**Tech Stack:** Apache Kafka Â· Apache Spark Streaming Â· HBase Â· Presto Â· Java Â· Protobuf Â· Kubernetes Â· Python

---

## ğŸ“Œ Overview

**SurveilNet** is a real-time trade surveillance platform built to detect suspicious financial activities such as spoofing, layering, and wash trading. It processes high-volume trade events, applies business logic to detect anomalies, and scales horizontally using modern big data and cloud-native technologies.

---

## ğŸ” Key Features

- âš¡ **Real-Time Data Processing:** Ingests millions of trade events per day using Apache Kafka and processes them via Spark Streaming.
- ğŸ“¦ **Compact Serialization:** Uses Protobuf for efficient and cross-platform data serialization.
- ğŸ§  **Anomaly Detection:** Identifies irregular trading behaviors (e.g., large-volume spikes, spoofing) and flags them for investigation.
- â˜¸ï¸ **Kubernetes Deployments:** Ready-to-run manifests for scalable deployment in cloud-native environments.
- ğŸ” **Presto Integration:** Allows fast, ad-hoc querying on enriched datasets for surveillance analytics.

---

## ğŸ§± Project Structure

SurveilNet/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ kafka_producer/ # Simulated Kafka trade event producer
â”‚ â””â”€â”€ spark_streaming/ # Spark job for real-time filtering and detection
â”œâ”€â”€ configs/ # Config files and schemas
â”œâ”€â”€ k8s/ # Kubernetes deployment manifests
â”œâ”€â”€ scripts/ # Optional helper scripts
â””â”€â”€ README.md # Project documentation

yaml
Copy
Edit

---

## ğŸ›  Setup Instructions

### ğŸ“¥ 1. Clone the Repository

```bash
git clone https://github.com/rahulrn9/SurveilNet.git
cd SurveilNet
âš™ï¸ 2. Start Kafka and Zookeeper
You can use Docker Compose or a local installation.

bash
Copy
Edit
docker run -d --name zookeeper -p 2181:2181 zookeeper
docker run -d --name kafka -p 9092:9092 \
  -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \
  -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \
  -e KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092 \
  --network host \
  confluentinc/cp-kafka
ğŸ§ª 3. Run Kafka Producer
Sends synthetic trade events to the Kafka topic trade-events.

bash
Copy
Edit
cd src/kafka_producer
python producer.py
ğŸ”„ 4. Run Spark Streaming Job
You can run locally or submit it to a Spark cluster:

bash
Copy
Edit
cd src/spark_streaming
spark-submit streaming_job.py
â˜¸ï¸ 5. Kubernetes Deployment
Deploy the Spark job in a Kubernetes cluster:

bash
Copy
Edit
kubectl apply -f k8s/deployment.yaml
Ensure the image and volume mounts are configured to match your cluster setup.

ğŸ“¦ Sample Trade Event Format
json
Copy
Edit
{
  "user_id": 1024,
  "symbol": "TSLA",
  "action": "BUY",
  "quantity": 800,
  "timestamp": 1720254012
}
ğŸ“Š Future Enhancements
Integrate HBase or Snowflake for historical data storage

Alerting system with Slack/Email/Webhooks for flagged trades

Add Prometheus + Grafana for monitoring Spark job metrics

Support for Flink or Beam for enhanced stream processing flexibility

ğŸ¤ Contributing
Pull requests and forks are welcome! Please raise an issue to discuss your ideas before opening a major PR.

